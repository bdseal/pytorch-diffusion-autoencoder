model:
  target: model.unet.UNet
  params:
    timestep_embed: 16
    size: [8, 8] # patch size
    in_channels: 1
    out_channels: 1
    base_hidden_channels: 64
    n_layers: 2
    inner_layers: [3,3]
    chan_multiplier: [1,2]
    attention_layers: [False, True]
    z_dim: 256

encoder:
  target: model.latent_encoder.LatentEncoder
  params:
    in_channels: 1
    base_hidden_channels: 64
    n_layers: 3
    chan_multiplier: [1,2,4]
    inner_layers: [3,3,3]
    linear_layers: [448, 512, 256]
    attention_layers: [False, True, False]
    z_dim: 256
    dropout: 0.1

diffusion:
  target: model.diffusion.PatchAutoEncoderGaussianDiffusion
  params:
    image_size: 32
    patch_size: 8
    timesteps: 1000

training:
  scheduler: none
  learning_rate: 1e-4
  encoder_learning_rate: 1e-4
  batch_size: 256
  save_every: 1000
  sample_every: 500
  grandient_accumulation_steps: 1
  grad_clip: 1
  ema_decay: 0.995
  fp16: true

data:
  target: data.mnist.QMNIST
  params:
    root: /media/lleonard/big_slow_disk/datasets/qmnist
    size: [32, 32]
